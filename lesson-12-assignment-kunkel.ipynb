{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8afad4eb-030e-426f-acbc-7c049d25e745",
   "metadata": {},
   "source": [
    "# Lesson 12 Assignment\n",
    "### Jessica Kunkel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00ca2caa-2256-4b52-9755-d3e0a2f09491",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\"\"\" Import display functions and create a function to print outputs to \n",
    "Markdown \"\"\"\n",
    "\n",
    "from IPython.display import display, Markdown\n",
    "def printmd(string):\n",
    "    display(Markdown(string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b02dbeb-8828-4c16-beb8-6c055d5c3b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Import the os module and set the working directory to where the \n",
    "mtcars.csv file is saved \"\"\"\n",
    "\n",
    "import os\n",
    "path = r\"C:\\Users\\jkunk\\OneDrive\\Documents\\_SWENG Masters\\9 DAAN 862 - Analytics Programming Python\"\n",
    "os.chdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2bccd202-4691-4b4c-8628-c43e24e41354",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Import NumPy, pandas, and dataframe \"\"\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85e50c3-60ee-45a7-a2b5-3d9baf14240c",
   "metadata": {},
   "source": [
    "### 1. Use the following codes to load the `assignment12.txt` which contains file names. How many file names are in it? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a288e9be-ba5d-4647-ac74-def68bd7f0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"Assignment_12.txt\" , 'r')\n",
    "\n",
    "text1 = file.read()\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96480dfc-d773-49f4-8c82-059253f5904b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "There are 90 file names in the `assignment12.txt` file"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# split string into array\n",
    "text_arr = text1.split()\n",
    "printmd(f'There are {len(text_arr)} file names in the `assignment12.txt` file')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8288e558-c6bc-45cc-be85-704e04affff4",
   "metadata": {},
   "source": [
    "### 2. Identify the pattern of the file names, and find out how many file names match the pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d41d6fba-6087-4d15-b1f4-fa77dbc30f5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "['arxiv_annotate10_7_1.txt', 'arxiv_annotate10_7_2.txt', 'arxiv_annotate10_7_3.txt', 'arxiv_annotate1_13_1.txt', 'arxiv_annotate1_13_2.txt', 'arxiv_annotate1_13_3.txt', 'arxiv_annotate2_66_1.txt', 'arxiv_annotate2_66_2.txt', 'arxiv_annotate2_66_3.txt', 'arxiv_annotate3_80_1.txt']"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "['jdm_annotate10_210_1.txt', 'jdm_annotate10_210_2.txt', 'jdm_annotate10_210_3.txt', 'jdm_annotate1_103_1.txt', 'jdm_annotate1_103_2.txt', 'jdm_annotate1_103_3.txt', 'jdm_annotate2_107_1.txt', 'jdm_annotate2_107_2.txt', 'jdm_annotate2_107_3.txt', 'jdm_ann^otate3_120_1.txt']"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "['plos_annotate3_798_2.txt', 'plos_annotate3_798_3.txt', 'plos_annotate4_1052_1.txt', 'plos_annotate4_1052_2.txt', 'plos_annotate4_1052_3.txt', 'plos_annotate5_1375_1.txt', 'plos_annotate5_1375_2.txt', 'plos_anno%tate5_1375_3.txt', 'plos_annotate6_1032_1.txt', 'plos_annotate6_1032_2.txt']"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# print samples of the file names\n",
    "printmd(f'{text_arr[0:10]}')\n",
    "printmd(f'{text_arr[30:40]}')\n",
    "printmd(f'{text_arr[70:80]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed463fde-10c5-47e1-b12d-d9ce6ec3e9a7",
   "metadata": {},
   "source": [
    "Based on inspection of a selection of the text array, I identified the following pattern: \n",
    "- lowercase string length 1 to 6\n",
    "- underscore ( _ )\n",
    "- 'annotate' followed by a number\n",
    "- underscore ( _ )\n",
    "- a number\n",
    "- underscore ( _ )\n",
    "- a number\n",
    "- .txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5569eb1d-c7cb-40a7-89e1-ca9d223f20f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Number of file names that match the pattern: 84"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import re package\n",
    "import re\n",
    "\n",
    "# define the pattern\n",
    "pattern = r'[a-z]+_+annotate+[0-9]+_+[0-9]+_+[0-9]+.txt'\n",
    "\n",
    "# compile the pattern, ignoring case\n",
    "regex = re.compile(pattern, flags=re.IGNORECASE)\n",
    "\n",
    "# generate list of matches based on the original text\n",
    "matches = regex.findall(text1)\n",
    "\n",
    "# print the number of files that match the pattern\n",
    "printmd(f'Number of file names that match the pattern: {len(matches)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f28bd81-83b6-4e09-ba53-e1f800070f6f",
   "metadata": {},
   "source": [
    "### 3. Find out file names that don't match with the pattern you designed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c186562-1628-41ab-9562-d844c8428078",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The file names that don't match the pattern I designed are as follows: ['jdm_ann^otate3_120_1.txt', 'jdm_anno&tate6_32_2.txt', 'jdm_annotat#e8_177_2.txt', 'plos_annotat*e1_6_2.txt', 'plos_anno%tate5_1375_3.txt', 'plos_annot@ate7_1233_2.txt']"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# initialize index array\n",
    "unmatched_loc = []\n",
    "\n",
    "# isolate file names that do not match the pattern\n",
    "for name in text_arr:\n",
    "    if name not in matches:\n",
    "        unmatched_loc.append(text_arr.index(name))\n",
    "\n",
    "# get file names from their indices\n",
    "unmatched_files = [text_arr[i] for i in unmatched_loc]\n",
    "unmatched_files\n",
    "\n",
    "# print the file names that don't match the pattern\n",
    "printmd(f'The file names that don\\'t match the pattern I designed are as follows: {unmatched_files}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ac7b90-1702-434b-9529-c981763900cd",
   "metadata": {},
   "source": [
    "### 4. Use the following codes to read the text from `arxiv_annotate1_13_1.txt`. Normalize the words and find out their counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "77549414-1874-4abf-882f-dadcc1f2336c",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"arxiv_annotate1_13_1.txt\", 'r')\n",
    "\n",
    "text2 = file.read()\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15344e4-3d0b-4e28-9ef0-a29ccfd83659",
   "metadata": {},
   "source": [
    "The standard NLP cleaning and normalization process includes:\n",
    "- split a text into a list of words\n",
    "- remove special characters\n",
    "- perform stemming and lemmatization on each word\n",
    "- find frequency of unique words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4765cc3-a04a-4a85-a47c-d26d44d7e259",
   "metadata": {},
   "source": [
    "First, I removed the special characters from the text. Then, I converted the text into a list of words (without special characters, this can be done using `.split()` or `nltk.word_tokenize()`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3267352b-62e8-46d3-9179-b0f3cf51ece0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean the text to remove special characters\n",
    "text2_clean = re.sub(r'[^a-zA-Z0-9\\s]', '', text2)\n",
    "\n",
    "# split the text into an array/list of words for manipulation\n",
    "text2_arr = text2_clean.lower().split()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641314f7-0bb9-43bc-8eae-75e0a46f041b",
   "metadata": {},
   "source": [
    "Next, I counted the total words and unique words in the text. I also printed a sample of the unique words in the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "79b3adf7-53c9-4413-b93a-684ef5a96782",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The `arxiv_annotate1_13_1.txt` file has 842 total words, and 333 unique words."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import NLTK package\n",
    "import nltk\n",
    "\n",
    "# Count vocabulary of words\n",
    "total_words_cnt = len(text2_arr)\n",
    "unique_words_cnt = len(set(text2_arr))\n",
    "\n",
    "printmd(f'The `arxiv_annotate1_13_1.txt` file has {total_words_cnt} total words, and {unique_words_cnt} unique words.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "876c51b6-2195-4fda-b358-b2ebacf93256",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['two',\n",
       " 'introduce',\n",
       " 'e',\n",
       " 'macroscopic',\n",
       " 'often',\n",
       " 'providers',\n",
       " 'high',\n",
       " 'employ',\n",
       " 'classifications',\n",
       " 'customers']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print a sample of the unique words in the text file\n",
    "list(set(text2_arr))[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27da091-a671-4905-b701-88a83a3dfbfe",
   "metadata": {},
   "source": [
    "Next, I got the unique words in the text and their frequencies. I printed a sample of the unique words from the frequency distribution, and found all words longer than 5 characters with frequency larger than 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1925666c-330e-4255-939c-0cd6a13bb7b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abstract',\n",
       " 'misc',\n",
       " 'although',\n",
       " 'the',\n",
       " 'internet',\n",
       " 'as',\n",
       " 'level',\n",
       " 'topology',\n",
       " 'has',\n",
       " 'been',\n",
       " 'extensively',\n",
       " 'studied',\n",
       " 'over',\n",
       " 'past',\n",
       " 'few',\n",
       " 'years',\n",
       " 'little',\n",
       " 'is',\n",
       " 'known',\n",
       " 'about']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Frequency of words\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "dist = FreqDist(text2_arr)\n",
    "\n",
    "# list of unique words\n",
    "words = dist.keys()\n",
    "\n",
    "list(words)[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b67af8c5-645c-4d3b-bc8d-4f57600275cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['internet', 'different', 'number']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freqwords = [w for w in words if len(w) > 5 and dist[w] > 10]\n",
    "freqwords[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8990de83-691b-4f8f-b097-503d78952ada",
   "metadata": {},
   "source": [
    "Next, I performed stemming on the text and printed a sample to compare the words to their base forms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "09adcc3d-ab3e-41ea-8dc7-5328fbd12218",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abstract',\n",
       " 'misc',\n",
       " 'although',\n",
       " 'the',\n",
       " 'internet',\n",
       " 'as',\n",
       " 'level',\n",
       " 'topolog',\n",
       " 'ha',\n",
       " 'been',\n",
       " 'extens',\n",
       " 'studi',\n",
       " 'over',\n",
       " 'past',\n",
       " 'few',\n",
       " 'year',\n",
       " 'littl',\n",
       " 'is',\n",
       " 'known',\n",
       " 'about']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "porter = nltk.PorterStemmer()\n",
    "\n",
    "stemmed = [porter.stem(w) for w in words]\n",
    "stemmed[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4ce09a-c1c4-4579-aa7a-c51b6ffc6d04",
   "metadata": {},
   "source": [
    "Last, I performed lemmatization on the text and printed a sample to compare to the original text and stemmed text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6519e93b-6c48-4bc2-bf4f-41abaf30398b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abstract',\n",
       " 'misc',\n",
       " 'although',\n",
       " 'the',\n",
       " 'internet',\n",
       " 'a',\n",
       " 'level',\n",
       " 'topology',\n",
       " 'ha',\n",
       " 'been',\n",
       " 'extensively',\n",
       " 'studied',\n",
       " 'over',\n",
       " 'past',\n",
       " 'few',\n",
       " 'year',\n",
       " 'little',\n",
       " 'is',\n",
       " 'known',\n",
       " 'about']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WNlemma = nltk.WordNetLemmatizer()\n",
    "\n",
    "lemmed = [WNlemma.lemmatize(w) for w in words]\n",
    "lemmed[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ead21e-ec34-406c-bf35-52f441bb6f71",
   "metadata": {},
   "source": [
    "Interestingly, the lemmatization process returned the word 'ha' as the root word of 'has', when I would have expected the root word to return 'has'. I would have also expected the root word of 'extensively' to have returned 'extensive', and 'studied' to return 'study'."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
